# -*- coding: utf-8 -*-
"""Final Year Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xmlWJD78XFxtFPAhtwQ0RePymkgFMpen
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

datasets = ['dataset13.csv', 'dataset14.csv', 'DataSet15.csv', 'DataSet16.csv']

for dataset_file in datasets:
    # Step 2: Load your dataset
    dataset = pd.read_csv(dataset_file)

# Assuming you have three variables: 'X1', 'X2', and 'y'
X = dataset[['X1', 'X2']]
y = dataset['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
# Predict the values
y_pred = model.predict(X_test)

    # Calculate the model's performance
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error for {dataset_file}: {mse}')

plt.scatter(y_test, y_pred)
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title(f'{dataset_file} - True Values vs Predictions')
plt.show()

coefficients = model.coef_
 intercept = model.intercept_
 print(f'Coefficients for {dataset_file}: {coefficients}')
 print(f'Intercept for {dataset_file}: {intercept}')

# Create the model equation
    feature_names = ['X1', 'X2']  # Assuming your features are named X1 and X2
    equation = f'y = {intercept:.2f}'
    for i, coef in enumerate(coefficients):
        equation += f' + {coef:.2f} * {feature_names[i]}'

    print(f'Model Equation for {dataset_file}: {equation}')

# Step 1: Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Assuming you have four datasets named dataset1.csv, dataset2.csv, dataset3.csv, and dataset4.csv
datasets = ['dataset13.csv', 'dataset14.csv', 'DataSet15.csv', 'DataSet16.csv']

for dataset_file in datasets:
    # Step 2: Load your dataset
    dataset = pd.read_csv(dataset_file)

    # Step 3: Explore and preprocess the data
    # You may need to perform data cleaning, feature engineering, and handle missing values.

    # Assuming you have three variables: 'X1', 'X2', and 'y'
    X = dataset[['X1', 'X2']]
    y = dataset['y']

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Step 4: Build a predictive model
    # Let's use Random Forest Regressor as an example
    model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters here
    model.fit(X_train, y_train)

    # Predict the values
    y_pred = model.predict(X_test)

    # Calculate the model's performance
    mse = mean_squared_error(y_test, y_pred)
    print(f'Mean Squared Error for {dataset_file}: {mse}')

    # Visualize the results
    plt.scatter(y_test, y_pred)
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.title(f'{dataset_file} - True Values vs Predictions')
    plt.show()

    # You can access feature importances if needed
    feature_importances = model.feature_importances_
    print(f'Feature Importances for {dataset_file}: {feature_importances}')


    summary = " + ".join([f"{feature} * {importance:.2f}" for feature, importance in zip(['X1', 'X2'], feature_importances)])

    print(f'Model Summary for {dataset_file}: y = {summary}')

# Step 1: Import necessary libraries
import pandas as pd
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Assuming you have four datasets named dataset1.csv, dataset2.csv, dataset3.csv, and dataset4.csv
datasets = ['dataset13.csv', 'dataset14.csv', 'DataSet15.csv', 'DataSet16.csv']

for dataset_file in datasets:
    # Step 2: Load your dataset
    dataset = pd.read_csv(dataset_file)

    # Step 3: Explore and preprocess the data
    # You may need to perform data cleaning, feature engineering, and handle missing values.

    # Assuming you have three variables: 'X1', 'X2', and 'y'
    X = dataset[['X1', 'X2']]
    y = dataset['y']

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Step 4: Build a predictive model
    # Let's use SVM Regressor as an example
    model = SVR(kernel='linear', C=1.0)  # You can adjust hyperparameters here
    model.fit(X_train, y_train)

    # Predict the values
    y_pred = model.predict(X_test)

    # Calculate the model's performance
    mse = mean_squared_error(y_test, y_pred)
    print(f'Mean Squared Error for {dataset_file}: {mse}')

    # Visualize the results
    plt.scatter(y_test, y_pred)
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.title(f'{dataset_file} - True Values vs Predictions')
    plt.show()

    # You can also get coefficients in the case of a linear kernel
    if model.kernel == 'linear':
        coefficients = model.coef_
        intercept = model.intercept_
        print(f'Coefficients for {dataset_file}: {coefficients}')
        print(f'Intercept for {dataset_file}: {intercept}')

    feature_names = ['X1', 'X2']  # Assuming your features are named X1 and X2
    equation = f'y = {intercept:.2f}'
    for i, coef in enumerate(coefficients):
        equation += f' + {coef:.2f} * {feature_names[i]}'

    print(f'Model Equation for {dataset_file}: {equation}')

# Step 1: Import necessary libraries
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Assuming you have four datasets named dataset1.csv, dataset2.csv, dataset3.csv, and dataset4.csv
datasets = ['dataset13.csv', 'dataset14.csv', 'DataSet15.csv', 'DataSet16.csv']

for dataset_file in datasets:
    # Step 2: Load your dataset
    dataset = pd.read_csv(dataset_file)

    # Step 3: Explore and preprocess the data
    # You may need to perform data cleaning, feature engineering, and handle missing values.

    # Assuming you have three variables: 'X1', 'X2', and 'y'
    X = dataset[['X1', 'X2']]
    y = dataset['y']

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Step 4: Build a predictive model
    # Let's use XGBoost as an example
    model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, random_state=42)
    # You can adjust hyperparameters here

    model.fit(X_train, y_train)

    # Predict the values
    y_pred = model.predict(X_test)

    # Calculate the model's performance
    mse = mean_squared_error(y_test, y_pred)
    print(f'Mean Squared Error for {dataset_file}: {mse}')

    # Visualize the results
    plt.scatter(y_test, y_pred)
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.title(f'{dataset_file} - True Values vs Predictions')
    plt.show()

    # You can also get feature importances if needed
    feature_importances = model.feature_importances_
    print(f'Feature Importances for {dataset_file}: {feature_importances}')

    feature_names = ['X1', 'X2']  # Assuming your features are named X1 and X2
    importance_summary = " + ".join([f"{feature} * {importance:.2f}" for feature, importance in zip(feature_names, feature_importances)])

    print(f'Feature Importance Summary for {dataset_file}: {importance_summary}')